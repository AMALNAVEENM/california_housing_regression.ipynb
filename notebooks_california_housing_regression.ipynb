{
  "metadata": {
    "kernelspec": {
      "name": "xpython",
      "display_name": "Python 3.13 (XPython)",
      "language": "python"
    },
    "language_info": {
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "version": "3.13.1"
    }
  },
  "nbformat_minor": 5,
  "nbformat": 4,
  "cells": [
    {
      "id": "f8fabbb3-5bbd-472d-b4b7-875974811d47",
      "cell_type": "code",
      "source": "# Import essential libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Scikit-learn imports\nfrom sklearn.datasets import fetch_california_housing\nfrom sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n\n# Set global style and seed for reproducibility\nplt.style.use('seaborn-v0_8')\nnp.random.seed(42)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "120e4c0f-b240-48e1-b103-3413eddafd39",
      "cell_type": "code",
      "source": "# Load the California Housing dataset\ncalifornia = fetch_california_housing()\nX = pd.DataFrame(california.data, columns=california.feature_names)\ny = pd.Series(california.target, name='MedHouseVal')\n\n# Combine into a single DataFrame for EDA\ndf = pd.concat([X, y], axis=1)\n\n# Display basic info\nprint(\"Dataset shape:\", df.shape)\nprint(\"\\nFirst 5 rows:\")\ndisplay(df.head())\n\n# Check for missing values\nprint(\"\\nMissing values per column:\")\nprint(df.isnull().sum())",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "60e1f605-afca-4291-8fae-fa0a1961a4f1",
      "cell_type": "code",
      "source": "# Summary statistics\nprint(\"Summary Statistics:\")\ndisplay(df.describe())\n\n# Plot feature distributions\nplt.figure(figsize=(15, 10))\nfor i, col in enumerate(df.columns):\n    plt.subplot(3, 3, i + 1)\n    sns.histplot(df[col], kde=True, bins=30, color='steelblue')\n    plt.title(f'{col}', fontsize=12)\nplt.tight_layout()\nplt.show()\n\n# Correlation heatmap\nplt.figure(figsize=(10, 8))\ncorrelation_matrix = df.corr()\nsns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f', linewidths=0.5)\nplt.title('Feature Correlation Matrix', fontsize=14)\nplt.show()",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "f00486ce-a0db-4658-885f-8a9734bd8d8b",
      "cell_type": "code",
      "source": "# Split the data (80% train, 20% test)\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42\n)\n\n# Standardize features (important for SVR and Linear Regression)\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\n# Convert back to DataFrames for clarity (optional but helpful)\nX_train_scaled = pd.DataFrame(X_train_scaled, columns=X.columns)\nX_test_scaled = pd.DataFrame(X_test_scaled, columns=X.columns)\n\nprint(\"Preprocessing complete. Data is split and scaled.\")",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "89cd75e8-679b-4d77-bbde-193764b02bee",
      "cell_type": "code",
      "source": "# Initialize models\nmodels = {\n    'Linear Regression': LinearRegression(),\n    'Decision Tree': DecisionTreeRegressor(random_state=42),\n    'Random Forest': RandomForestRegressor(random_state=42),\n    'Gradient Boosting': GradientBoostingRegressor(random_state=42),\n    'SVR': SVR()\n}\n\n# Store predictions\nbase_predictions = {}\n\n# Train and predict\nfor name, model in models.items():\n    if name in ['Linear Regression', 'SVR']:\n        # Use scaled data\n        model.fit(X_train_scaled, y_train)\n        pred = model.predict(X_test_scaled)\n    else:\n        # Tree-based models: use original scale\n        model.fit(X_train, y_train)\n        pred = model.predict(X_test)\n    base_predictions[name] = pred\n\nprint(\"All base models trained and predictions generated.\")",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "140e3128-e031-40b3-ace6-7d534a29f9f1",
      "cell_type": "code",
      "source": "# Compute evaluation metrics\nresults = {}\nfor name, pred in base_predictions.items():\n    mse = mean_squared_error(y_test, pred)\n    mae = mean_absolute_error(y_test, pred)\n    r2 = r2_score(y_test, pred)\n    results[name] = {'MSE': mse, 'MAE': mae, 'R¬≤': r2}\n\n# Display results\nresults_df = pd.DataFrame(results).T.round(4)\nresults_df = results_df.sort_values('R¬≤', ascending=False)\nprint(\"Base Model Performance:\")\ndisplay(results_df)\n\n# Identify best and worst\nbest_base = results_df.index[0]\nworst_base = results_df.index[-1]\nprint(f\"\\nBest base model: {best_base} (R¬≤ = {results_df.loc[best_base, 'R¬≤']:.4f})\")\nprint(f\"Worst base model: {worst_base} (R¬≤ = {results_df.loc[worst_base, 'R¬≤']:.4f})\")",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "c13e69d4-76c5-4e2e-8f6c-0b158da0b70d",
      "cell_type": "code",
      "source": "# Perform 5-fold cross-validation\ncv_results = {}\nfor name, model in models.items():\n    X_cv = X_train_scaled if name in ['Linear Regression', 'SVR'] else X_train\n    scores = cross_val_score(model, X_cv, y_train, cv=5, scoring='r2')\n    cv_results[name] = scores.mean()\n\n# Display CV scores\ncv_df = pd.DataFrame(list(cv_results.items()), columns=['Model', 'CV R¬≤']).sort_values('CV R¬≤', ascending=False)\nprint(\"5-Fold Cross-Validation R¬≤ Scores:\")\ndisplay(cv_df.round(4))",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "02f15974-4545-4e0a-8fe2-0ad4c435664d",
      "cell_type": "code",
      "source": "# Define parameter grids\nparam_grids = {\n    'Decision Tree': {\n        'max_depth': [10, 20, None],\n        'min_samples_split': [2, 5, 10]\n    },\n    'Random Forest': {\n        'n_estimators': [100, 200],\n        'max_depth': [10, 20, None]\n    },\n    'Gradient Boosting': {\n        'n_estimators': [100, 200],\n        'learning_rate': [0.05, 0.1],\n        'max_depth': [3, 5]\n    },\n    'SVR': {\n        'C': [1, 10],\n        'gamma': ['scale', 'auto'],\n        'epsilon': [0.1, 0.2]\n    }\n}\n\n# Tune models\ntuned_models = {}\nprint(\"Starting hyperparameter tuning...\\n\")\n\nfor name in ['Decision Tree', 'Random Forest', 'Gradient Boosting', 'SVR']:\n    print(f\"Tuning {name}...\")\n    grid = GridSearchCV(\n        models[name],\n        param_grids[name],\n        cv=3,\n        scoring='r2',\n        n_jobs=-1,\n        verbose=0\n    )\n    if name == 'SVR':\n        grid.fit(X_train_scaled, y_train)\n    else:\n        grid.fit(X_train, y_train)\n    tuned_models[name] = grid.best_estimator_\n    print(f\"  Best CV R¬≤: {grid.best_score_:.4f}\")\n    print(f\"  Best params: {grid.best_params_}\\n\")\n\n# Add Linear Regression (no tuning needed)\nlr = LinearRegression()\nlr.fit(X_train_scaled, y_train)\ntuned_models['Linear Regression'] = lr",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "52839ec0-8860-4cc2-8f4d-e19a70082cc9",
      "cell_type": "code",
      "source": "# Generate predictions with tuned models\ntuned_predictions = {}\nfor name, model in tuned_models.items():\n    if name in ['Linear Regression', 'SVR']:\n        pred = model.predict(X_test_scaled)\n    else:\n        pred = model.predict(X_test)\n    tuned_predictions[name] = pred\n\n# Evaluate tuned models\ntuned_results = {}\nfor name, pred in tuned_predictions.items():\n    mse = mean_squared_error(y_test, pred)\n    mae = mean_absolute_error(y_test, pred)\n    r2 = r2_score(y_test, pred)\n    tuned_results[name] = {'MSE': mse, 'MAE': mae, 'R¬≤': r2}\n\n# Display final results\nfinal_df = pd.DataFrame(tuned_results).T.round(4)\nfinal_df = final_df.sort_values('R¬≤', ascending=False)\nprint(\"Final Model Performance After Tuning:\")\ndisplay(final_df)\n\n# Identify the best model\nbest_model_name = final_df.index[0]\nprint(f\"\\nüèÜ Best Model: {best_model_name}\")\nprint(f\"   R¬≤: {final_df.loc[best_model_name, 'R¬≤']:.4f}\")\nprint(f\"   MSE: {final_df.loc[best_model_name, 'MSE']:.4f}\")\nprint(f\"   MAE: {final_df.loc[best_model_name, 'MAE']:.4f}\")",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}